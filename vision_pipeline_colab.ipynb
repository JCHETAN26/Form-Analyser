{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üèãÔ∏è Fitness-AQA Vision Pipeline (Google Colab)\n",
        "\n",
        "This notebook extracts **2D pose keypoints** from exercise videos using **MMPose**.\n",
        "\n",
        "## üìã What This Does:\n",
        "1. Checks Python version compatibility\n",
        "2. Installs MMPose and dependencies (with version pinning)\n",
        "3. Uploads your video (or uses a sample)\n",
        "4. Extracts 17 COCO keypoints per frame\n",
        "5. Applies Savitzky-Golay smoothing\n",
        "6. Normalizes coordinates (optional)\n",
        "7. Saves output as `.json` for the modeling team\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Setup Instructions:\n",
        "1. **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**\n",
        "2. Run all cells in order\n",
        "3. Upload your video when prompted\n",
        "4. Download the output JSON\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üêç Step 1: Check Python Version\n",
        "\n",
        "MMPose works best with Python 3.8-3.10. Colab typically uses 3.10."
      ],
      "metadata": {
        "id": "python_version_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(f\"Current Python version: {sys.version}\")\n",
        "\n",
        "python_version = sys.version_info\n",
        "if python_version.major == 3 and 8 <= python_version.minor <= 10:\n",
        "    print(\"‚úÖ Python version is compatible with MMPose!\")\n",
        "elif python_version.major == 3 and python_version.minor > 10:\n",
        "    print(\"‚ö†Ô∏è  Python version might be too new. Installing compatibility fixes...\")\n",
        "else:\n",
        "    print(\"‚ùå Python version incompatible. Please use Python 3.8-3.10.\")"
      ],
      "metadata": {
        "id": "check_python_version"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì¶ Step 2: Install Dependencies\n",
        "\n",
        "Installing MMPose with version pinning for maximum compatibility."
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade pip and setuptools\n",
        "!pip install --upgrade pip setuptools wheel -q\n",
        "\n",
        "# Install OpenMIM\n",
        "!pip install -U openmim -q\n",
        "\n",
        "# Install MMPose stack with version constraints\n",
        "!mim install mmengine -q\n",
        "!mim install \"mmcv>=2.0.0,<2.2.0\" -q\n",
        "!mim install \"mmdet>=3.0.0\" -q\n",
        "!mim install \"mmpose>=1.0.0\" -q\n",
        "\n",
        "# Install signal processing libraries\n",
        "!pip install scipy opencv-python matplotlib -q\n",
        "\n",
        "# Pin numpy to avoid binary incompatibility\n",
        "!pip install \"numpy<2.0.0\" -q\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì§ Step 3: Upload Your Video\n",
        "\n",
        "Click \"Choose Files\" and upload your `.mp4` video."
      ],
      "metadata": {
        "id": "upload_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "video_path = list(uploaded.keys())[0]\n",
        "print(f\"‚úÖ Uploaded: {video_path}\")"
      ],
      "metadata": {
        "id": "upload_video"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Step 4: Define the Vision Pipeline\n",
        "\n",
        "The same `PoseExtractor` class from your local `video_processor.py`."
      ],
      "metadata": {
        "id": "pipeline_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import logging\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.signal import savgol_filter\n",
        "from mmpose.apis import MMPoseInferencer\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class PoseExtractor:\n",
        "    def __init__(self, mode='human', device='cuda'):\n",
        "        logger.info(f\"Initializing MMPose (device={device})...\")\n",
        "        self.inferencer = MMPoseInferencer(mode, device=device)\n",
        "\n",
        "    def smooth_signal(self, keypoints, window_length=5, polyorder=2):\n",
        "        logger.info(\"Applying Savitzky-Golay smoothing...\")\n",
        "        if len(keypoints) < window_length:\n",
        "            logger.warning(f\"Not enough frames ({len(keypoints)}) for smoothing. Returning raw.\")\n",
        "            return keypoints\n",
        "            \n",
        "        smoothed = np.zeros_like(keypoints)\n",
        "        for i in range(keypoints.shape[1]):\n",
        "            smoothed[:, i, 0] = savgol_filter(keypoints[:, i, 0], window_length, polyorder)\n",
        "            smoothed[:, i, 1] = savgol_filter(keypoints[:, i, 1], window_length, polyorder)\n",
        "        return smoothed\n",
        "\n",
        "    def normalize_signal(self, keypoints):\n",
        "        logger.info(\"Normalizing based on torso length...\")\n",
        "        normalized = np.zeros_like(keypoints)\n",
        "        \n",
        "        for f in range(len(keypoints)):\n",
        "            frame_kps = keypoints[f]\n",
        "            mid_shoulder = (frame_kps[5] + frame_kps[6]) / 2\n",
        "            mid_hip = (frame_kps[11] + frame_kps[12]) / 2\n",
        "            torso_len = np.linalg.norm(mid_shoulder - mid_hip)\n",
        "            \n",
        "            scale = 1.0 if torso_len < 1e-3 else 1.0 / torso_len\n",
        "            normalized[f] = (frame_kps - mid_hip) * scale\n",
        "        return normalized\n",
        "\n",
        "    def process_video(self, video_path, output_path=None):\n",
        "        if not os.path.exists(video_path):\n",
        "            raise FileNotFoundError(f\"Video {video_path} not found\")\n",
        "            \n",
        "        logger.info(f\"Processing: {video_path}\")\n",
        "        result_generator = self.inferencer(video_path, return_vis=False)\n",
        "        \n",
        "        raw_keypoints, scores = [], []\n",
        "        for result in result_generator:\n",
        "            preds = result['predictions']\n",
        "            if preds and len(preds) > 0:\n",
        "                raw_keypoints.append(preds[0]['keypoints'])\n",
        "                scores.append(preds[0]['keypoint_scores'])\n",
        "            else:\n",
        "                raw_keypoints.append(np.zeros((17, 2)))\n",
        "                scores.append(np.zeros(17))\n",
        "\n",
        "        raw_keypoints = np.array(raw_keypoints)\n",
        "        scores = np.array(scores)\n",
        "        \n",
        "        logger.info(f\"Extracted {len(raw_keypoints)} frames\")\n",
        "        smoothed = self.smooth_signal(raw_keypoints)\n",
        "        normalized = self.normalize_signal(smoothed)\n",
        "        \n",
        "        data = {\n",
        "            \"video_id\": os.path.basename(video_path),\n",
        "            \"frame_count\": len(raw_keypoints),\n",
        "            \"raw_keypoints\": raw_keypoints.tolist(),\n",
        "            \"smoothed_keypoints\": smoothed.tolist(),\n",
        "            \"normalized_keypoints\": normalized.tolist(),\n",
        "            \"scores\": scores.tolist()\n",
        "        }\n",
        "        \n",
        "        if output_path:\n",
        "            with open(output_path, 'w') as f:\n",
        "                json.dump(data, f)\n",
        "            logger.info(f\"Saved to {output_path}\")\n",
        "        return data\n",
        "\n",
        "print(\"‚úÖ PoseExtractor loaded!\")"
      ],
      "metadata": {
        "id": "define_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üöÄ Step 5: Run the Pipeline"
      ],
      "metadata": {
        "id": "run_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = PoseExtractor(mode='human', device='cuda')\n",
        "output_file = 'analysis.json'\n",
        "result = extractor.process_video(video_path, output_path=output_file)\n",
        "\n",
        "print(f\"\\n‚úÖ Processing complete!\")\n",
        "print(f\"üìä Frames: {result['frame_count']}\")\n",
        "print(f\"üíæ Saved to: {output_file}\")"
      ],
      "metadata": {
        "id": "run_pipeline"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Step 6: Visualize Results"
      ],
      "metadata": {
        "id": "viz_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "ret, frame = cap.read()\n",
        "cap.release()\n",
        "\n",
        "if ret:\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    keypoints = np.array(result['smoothed_keypoints'][0])\n",
        "    \n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.imshow(frame_rgb)\n",
        "    plt.scatter(keypoints[:, 0], keypoints[:, 1], c='red', s=100, marker='o', edgecolors='white', linewidths=2)\n",
        "    \n",
        "    for i, (x, y) in enumerate(keypoints):\n",
        "        plt.text(x, y, str(i), color='yellow', fontsize=10, ha='center', va='center', weight='bold')\n",
        "    \n",
        "    plt.title(\"Frame 0 - Detected Keypoints (Smoothed)\", fontsize=16)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nKeypoint Index Reference:\")\n",
        "    for i, name in enumerate([\"Nose\", \"L-Eye\", \"R-Eye\", \"L-Ear\", \"R-Ear\", \"L-Shoulder\", \"R-Shoulder\",\n",
        "                               \"L-Elbow\", \"R-Elbow\", \"L-Wrist\", \"R-Wrist\", \"L-Hip\", \"R-Hip\",\n",
        "                               \"L-Knee\", \"R-Knee\", \"L-Ankle\", \"R-Ankle\"]):\n",
        "        print(f\"  {i}: {name}\")"
      ],
      "metadata": {
        "id": "visualize"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üíæ Step 7: Download Output"
      ],
      "metadata": {
        "id": "download_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('analysis.json')\n",
        "print(\"‚úÖ Download started! Check your browser downloads.\")"
      ],
      "metadata": {
        "id": "download"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà (Optional) Plot Trajectory"
      ],
      "metadata": {
        "id": "trajectory_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot left wrist Y-coordinate over time\n",
        "left_wrist_idx = 9\n",
        "raw_y = [kp[left_wrist_idx][1] for kp in result['raw_keypoints']]\n",
        "smoothed_y = [kp[left_wrist_idx][1] for kp in result['smoothed_keypoints']]\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(raw_y, 'r-', alpha=0.4, linewidth=1, label='Raw (Jittery)')\n",
        "plt.plot(smoothed_y, 'b-', linewidth=2.5, label='Smoothed')\n",
        "plt.xlabel('Frame', fontsize=12)\n",
        "plt.ylabel('Y Coordinate (pixels)', fontsize=12)\n",
        "plt.title('Left Wrist Movement - Smoothing Effect', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Smoothing removes camera shake while preserving actual motion.\")"
      ],
      "metadata": {
        "id": "plot_trajectory"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## ‚úÖ Complete!\n",
        "\n",
        "**What you have:**\n",
        "- ‚úÖ `analysis.json` ready for Vishal\n",
        "- ‚úÖ Visualization confirming extraction quality\n",
        "- ‚úÖ Smoothing comparison showing signal processing\n",
        "\n",
        "**Next steps:**\n",
        "1. Send `analysis.json` to Vishal\n",
        "2. Point him to `HANDOFF_TO_VISHAL.md` on GitHub\n",
        "3. He can now build his model data loader!\n",
        "\n",
        "**GitHub:** https://github.com/JCHETAN26/Form-Analyser\n"
      ],
      "metadata": {
        "id": "done"
      }
    }
  ]
}

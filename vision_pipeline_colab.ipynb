{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "gpuType": "T4"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "language_info": {
            "name": "python"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üèãÔ∏è Fitness-AQA Vision Pipeline (Google Colab)\n",
                "\n",
                "This notebook extracts **2D pose keypoints** from exercise videos using **MMPose**.\n",
                "\n",
                "## üìã What This Does:\n",
                "1. Installs MMPose and dependencies\n",
                "2. Uploads your video (or uses a sample)\n",
                "3. Extracts 17 COCO keypoints per frame\n",
                "4. Applies Savitzky-Golay smoothing\n",
                "5. Normalizes coordinates (optional)\n",
                "6. Saves output as `.json` for the modeling team\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è Setup Instructions:\n",
                "1. **Runtime ‚Üí Change runtime type ‚Üí GPU (T4)**\n",
                "2. Run all cells in order\n",
                "3. Upload your video when prompted\n",
                "4. Download the output JSON\n",
                "\n",
                "---"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üì¶ Step 1: Install Dependencies\n",
                "\n",
                "This cell installs MMPose, MMDetection, and required libraries."
            ],
            "metadata": {
                "id": "install_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "!pip install -U openmim\n",
                "!mim install mmengine \"mmcv>=2.0.0\" \"mmdet>=3.0.0\" \"mmpose>=1.0.0\"\n",
                "!pip install scipy opencv-python matplotlib"
            ],
            "metadata": {
                "id": "install_deps"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üì§ Step 2: Upload Your Video\n",
                "\n",
                "Click the \"Choose Files\" button and upload your `.mp4` video."
            ],
            "metadata": {
                "id": "upload_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "uploaded = files.upload()\n",
                "video_path = list(uploaded.keys())[0]\n",
                "print(f\"‚úÖ Uploaded: {video_path}\")"
            ],
            "metadata": {
                "id": "upload_video"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîß Step 3: Define the Vision Pipeline\n",
                "\n",
                "This is the same `PoseExtractor` class from your local `video_processor.py`."
            ],
            "metadata": {
                "id": "pipeline_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import json\n",
                "import logging\n",
                "import numpy as np\n",
                "import cv2\n",
                "from scipy.signal import savgol_filter\n",
                "from mmpose.apis import MMPoseInferencer\n",
                "\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "class PoseExtractor:\n",
                "    def __init__(self, mode='human', device='cuda'):\n",
                "        logger.info(f\"Initializing MMPoseInferencer (mode={mode}, device={device})...\")\n",
                "        self.inferencer = MMPoseInferencer(mode, device=device)\n",
                "\n",
                "    def smooth_signal(self, keypoints, window_length=5, polyorder=2):\n",
                "        logger.info(\"Applying Savitzky-Golay smoothing...\")\n",
                "        if len(keypoints) < window_length:\n",
                "            logger.warning(f\"Not enough frames to smooth (got {len(keypoints)}, need {window_length}). Returning raw.\")\n",
                "            return keypoints\n",
                "            \n",
                "        smoothed_keypoints = np.zeros_like(keypoints)\n",
                "        num_points = keypoints.shape[1]\n",
                "        \n",
                "        for i in range(num_points):\n",
                "            smoothed_keypoints[:, i, 0] = savgol_filter(keypoints[:, i, 0], window_length, polyorder)\n",
                "            smoothed_keypoints[:, i, 1] = savgol_filter(keypoints[:, i, 1], window_length, polyorder)\n",
                "            \n",
                "        return smoothed_keypoints\n",
                "\n",
                "    def normalize_signal(self, keypoints):\n",
                "        logger.info(\"Normalizing signal based on torso length...\")\n",
                "        normalized_keypoints = np.zeros_like(keypoints)\n",
                "        \n",
                "        for f in range(len(keypoints)):\n",
                "            frame_kps = keypoints[f]\n",
                "            l_shoulder = frame_kps[5]\n",
                "            r_shoulder = frame_kps[6]\n",
                "            l_hip = frame_kps[11]\n",
                "            r_hip = frame_kps[12]\n",
                "            \n",
                "            mid_shoulder = (l_shoulder + r_shoulder) / 2\n",
                "            mid_hip = (l_hip + r_hip) / 2\n",
                "            torso_len = np.linalg.norm(mid_shoulder - mid_hip)\n",
                "            \n",
                "            if torso_len < 1e-3:\n",
                "                scale = 1.0\n",
                "            else:\n",
                "                scale = 1.0 / torso_len\n",
                "            \n",
                "            centered = frame_kps - mid_hip\n",
                "            normalized_keypoints[f] = centered * scale\n",
                "            \n",
                "        return normalized_keypoints\n",
                "\n",
                "    def process_video(self, video_path, output_path=None, visualize=False):\n",
                "        if not os.path.exists(video_path):\n",
                "            raise FileNotFoundError(f\"Video {video_path} not found.\")\n",
                "            \n",
                "        logger.info(f\"Processing video: {video_path}\")\n",
                "        result_generator = self.inferencer(video_path, return_vis=visualize)\n",
                "        \n",
                "        raw_keypoints = []\n",
                "        scores = []\n",
                "        \n",
                "        for result in result_generator:\n",
                "            preds = result['predictions']\n",
                "            if preds and len(preds) > 0:\n",
                "                raw_keypoints.append(preds[0]['keypoints'])\n",
                "                scores.append(preds[0]['keypoint_scores'])\n",
                "            else:\n",
                "                raw_keypoints.append(np.zeros((17, 2)))\n",
                "                scores.append(np.zeros(17))\n",
                "\n",
                "        raw_keypoints = np.array(raw_keypoints)\n",
                "        scores = np.array(scores)\n",
                "        \n",
                "        logger.info(f\"Raw data shape: {raw_keypoints.shape}\")\n",
                "        smoothed_keypoints = self.smooth_signal(raw_keypoints)\n",
                "        normalized_keypoints = self.normalize_signal(smoothed_keypoints)\n",
                "        \n",
                "        data_packet = {\n",
                "            \"video_id\": os.path.basename(video_path),\n",
                "            \"frame_count\": len(raw_keypoints),\n",
                "            \"raw_keypoints\": raw_keypoints.tolist(),\n",
                "            \"smoothed_keypoints\": smoothed_keypoints.tolist(),\n",
                "            \"normalized_keypoints\": normalized_keypoints.tolist(),\n",
                "            \"scores\": scores.tolist()\n",
                "        }\n",
                "        \n",
                "        if output_path:\n",
                "            with open(output_path, 'w') as f:\n",
                "                json.dump(data_packet, f)\n",
                "            logger.info(f\"Saved processed data to {output_path}\")\n",
                "            \n",
                "        return data_packet\n",
                "\n",
                "print(\"‚úÖ PoseExtractor class loaded!\")"
            ],
            "metadata": {
                "id": "define_pipeline"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üöÄ Step 4: Run the Pipeline\n",
                "\n",
                "This cell processes your video and saves the output as `analysis.json`."
            ],
            "metadata": {
                "id": "run_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Initialize the extractor (uses GPU if available)\n",
                "extractor = PoseExtractor(mode='human', device='cuda')\n",
                "\n",
                "# Process the video\n",
                "output_file = 'analysis.json'\n",
                "result = extractor.process_video(video_path, output_path=output_file, visualize=False)\n",
                "\n",
                "print(f\"\\n‚úÖ Processing complete!\")\n",
                "print(f\"üìä Frames processed: {result['frame_count']}\")\n",
                "print(f\"üíæ Output saved to: {output_file}\")"
            ],
            "metadata": {
                "id": "run_pipeline"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üìä Step 5: Preview the Results\n",
                "\n",
                "Let's visualize a single frame to verify the extraction worked."
            ],
            "metadata": {
                "id": "preview_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Load the video and extract frame 0\n",
                "cap = cv2.VideoCapture(video_path)\n",
                "ret, frame = cap.read()\n",
                "cap.release()\n",
                "\n",
                "if ret:\n",
                "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
                "    \n",
                "    # Get keypoints for frame 0\n",
                "    keypoints = np.array(result['smoothed_keypoints'][0])\n",
                "    \n",
                "    # Plot\n",
                "    plt.figure(figsize=(12, 8))\n",
                "    plt.imshow(frame_rgb)\n",
                "    plt.scatter(keypoints[:, 0], keypoints[:, 1], c='red', s=50, marker='o')\n",
                "    \n",
                "    # Annotate keypoints\n",
                "    keypoint_names = [\n",
                "        \"Nose\", \"L-Eye\", \"R-Eye\", \"L-Ear\", \"R-Ear\",\n",
                "        \"L-Shoulder\", \"R-Shoulder\", \"L-Elbow\", \"R-Elbow\",\n",
                "        \"L-Wrist\", \"R-Wrist\", \"L-Hip\", \"R-Hip\",\n",
                "        \"L-Knee\", \"R-Knee\", \"L-Ankle\", \"R-Ankle\"\n",
                "    ]\n",
                "    \n",
                "    for i, (x, y) in enumerate(keypoints):\n",
                "        plt.text(x, y, str(i), color='yellow', fontsize=8, ha='center', va='center')\n",
                "    \n",
                "    plt.title(\"Frame 0 - Detected Keypoints (Smoothed)\")\n",
                "    plt.axis('off')\n",
                "    plt.tight_layout()\n",
                "    plt.show()\n",
                "    \n",
                "    print(\"\\nüìå Keypoint Reference:\")\n",
                "    for i, name in enumerate(keypoint_names):\n",
                "        print(f\"  {i}: {name}\")\n",
                "else:\n",
                "    print(\"‚ùå Failed to read video frame\")"
            ],
            "metadata": {
                "id": "preview_results"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üíæ Step 6: Download the Output\n",
                "\n",
                "Click the download link to get `analysis.json` for Vishal."
            ],
            "metadata": {
                "id": "download_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import files\n",
                "\n",
                "files.download('analysis.json')\n",
                "print(\"‚úÖ Download started! Check your browser's download folder.\")"
            ],
            "metadata": {
                "id": "download_output"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üìà (Optional) Step 7: Plot Trajectory\n",
                "\n",
                "Visualize how a single keypoint moves over time (useful for debugging)."
            ],
            "metadata": {
                "id": "trajectory_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Plot the Y-coordinate of the left wrist over time\n",
                "left_wrist_idx = 9\n",
                "raw_y = [kp[left_wrist_idx][1] for kp in result['raw_keypoints']]\n",
                "smoothed_y = [kp[left_wrist_idx][1] for kp in result['smoothed_keypoints']]\n",
                "\n",
                "plt.figure(figsize=(12, 5))\n",
                "plt.plot(raw_y, 'r-', alpha=0.3, label='Raw (Jittery)')\n",
                "plt.plot(smoothed_y, 'b-', linewidth=2, label='Smoothed')\n",
                "plt.xlabel('Frame')\n",
                "plt.ylabel('Y Coordinate (pixels)')\n",
                "plt.title('Left Wrist Movement - Raw vs Smoothed')\n",
                "plt.legend()\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüìä This shows how smoothing removes camera jitter while preserving motion.\")"
            ],
            "metadata": {
                "id": "plot_trajectory"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "\n",
                "## ‚úÖ Done!\n",
                "\n",
                "You now have:\n",
                "1. ‚úÖ **`analysis.json`** - Ready to send to Vishal\n",
                "2. ‚úÖ **Visualization** - Confirming the pipeline works\n",
                "3. ‚úÖ **Smoothing comparison** - Showing signal quality\n",
                "\n",
                "### üì¨ Next Steps:\n",
                "- Share `analysis.json` with Vishal\n",
                "- Point him to `HANDOFF_TO_VISHAL.md` in the GitHub repo\n",
                "- He can now start building his model data loader!\n",
                "\n",
                "**GitHub Repo:** https://github.com/JCHETAN26/Form-Analyser\n"
            ],
            "metadata": {
                "id": "done_header"
            }
        }
    ]
}